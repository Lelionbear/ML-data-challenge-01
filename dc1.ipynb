{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge\n",
    "\n",
    "## Arturo Galvan-Alarcon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL: create a model to predict flight delays (ARR_DEL15 boolean target)\n",
    "\n",
    "I will do three approaches:\n",
    "\n",
    "* APPROACH 1 - ALL DATA\n",
    "* APPROACH 2 - USEFUL FEATURES FROM COORELATION MATRIX\n",
    "* APPROACH 3 - USEFUL FEATURES FROM FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('aggregated.csv', nrows=100000)# 5 129 354\n",
    "df = pd.read_csv('aggregated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "      <th>FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ORIGIN_CITY_NAME</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEST_CITY_NAME</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>ARR_DEL15</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>B6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>MCO</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark, NJ</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>B6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>MCO</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark, NJ</td>\n",
       "      <td>739.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>MCO</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark, NJ</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>MCO</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark, NJ</td>\n",
       "      <td>739.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>B6</td>\n",
       "      <td>33.0</td>\n",
       "      <td>BTV</td>\n",
       "      <td>Burlington, VT</td>\n",
       "      <td>JFK</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MONTH  DAY_OF_WEEK UNIQUE_CARRIER  FL_NUM ORIGIN ORIGIN_CITY_NAME DEST  \\\n",
       "0    2.0          6.0             B6    28.0    MCO      Orlando, FL  EWR   \n",
       "1    2.0          7.0             B6    28.0    MCO      Orlando, FL  EWR   \n",
       "2    2.0          1.0             B6    28.0    MCO      Orlando, FL  EWR   \n",
       "3    2.0          2.0             B6    28.0    MCO      Orlando, FL  EWR   \n",
       "4    2.0          3.0             B6    33.0    BTV   Burlington, VT  JFK   \n",
       "\n",
       "  DEST_CITY_NAME  CRS_DEP_TIME  ARR_DEL15  CRS_ELAPSED_TIME  DISTANCE    YEAR  \\\n",
       "0     Newark, NJ        1000.0        0.0             156.0     937.0  2017.0   \n",
       "1     Newark, NJ         739.0        0.0             153.0     937.0  2017.0   \n",
       "2     Newark, NJ        1028.0        0.0             158.0     937.0  2017.0   \n",
       "3     Newark, NJ         739.0        0.0             153.0     937.0  2017.0   \n",
       "4   New York, NY        1907.0        0.0              90.0     266.0  2017.0   \n",
       "\n",
       "    DAY  \n",
       "0  25.0  \n",
       "1  26.0  \n",
       "2  27.0  \n",
       "3  28.0  \n",
       "4   1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = df['FL_DATE'].str.split('-', n = 2, expand = True).astype('float64')\n",
    "df['YEAR'] = date[0]\n",
    "df['DAY'] = date[2]\n",
    "\n",
    "\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.drop([\"Unnamed: 13\", \"FL_DATE\"], axis=1).dropna()\n",
    "\n",
    "\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5058334 entries, 0 to 5129353\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   MONTH             float64\n",
      " 1   DAY_OF_WEEK       float64\n",
      " 2   UNIQUE_CARRIER    object \n",
      " 3   FL_NUM            float64\n",
      " 4   ORIGIN            object \n",
      " 5   ORIGIN_CITY_NAME  object \n",
      " 6   DEST              object \n",
      " 7   DEST_CITY_NAME    object \n",
      " 8   CRS_DEP_TIME      float64\n",
      " 9   ARR_DEL15         float64\n",
      " 10  CRS_ELAPSED_TIME  float64\n",
      " 11  DISTANCE          float64\n",
      " 12  YEAR              float64\n",
      " 13  DAY               float64\n",
      "dtypes: float64(9), object(5)\n",
      "memory usage: 578.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODE columns with object values into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    from sklearn import preprocessing\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    return le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_encoder = encode_labels(df.UNIQUE_CARRIER)\n",
    "df.UNIQUE_CARRIER = carrier_encoder.transform(df.UNIQUE_CARRIER).astype('float64')\n",
    "\n",
    "\n",
    "origin_encoder = encode_labels(df.ORIGIN)\n",
    "df.ORIGIN = origin_encoder.transform(df.ORIGIN).astype('float64')\n",
    "\n",
    "origin_city_encoder = encode_labels(df.ORIGIN_CITY_NAME)\n",
    "df.ORIGIN_CITY_NAME = origin_city_encoder.transform(df.ORIGIN_CITY_NAME).astype('float64')\n",
    "\n",
    "\n",
    "dest_encoder = encode_labels(df.DEST)\n",
    "df.DEST = dest_encoder.transform(df.DEST).astype('float64')\n",
    "\n",
    "dest_city_encoder = encode_labels(df.DEST_CITY_NAME)\n",
    "df.DEST_CITY_NAME = dest_city_encoder.transform(df.DEST_CITY_NAME).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5058334 entries, 0 to 5129353\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   MONTH             float64\n",
      " 1   DAY_OF_WEEK       float64\n",
      " 2   UNIQUE_CARRIER    float64\n",
      " 3   FL_NUM            float64\n",
      " 4   ORIGIN            float64\n",
      " 5   ORIGIN_CITY_NAME  float64\n",
      " 6   DEST              float64\n",
      " 7   DEST_CITY_NAME    float64\n",
      " 8   CRS_DEP_TIME      float64\n",
      " 9   ARR_DEL15         float64\n",
      " 10  CRS_ELAPSED_TIME  float64\n",
      " 11  DISTANCE          float64\n",
      " 12  YEAR              float64\n",
      " 13  DAY               float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 578.9 MB\n"
     ]
    }
   ],
   "source": [
    "master_df = df.copy()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB(train_x, train_y, test_x, test_y):\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'tree_method':\"hist\", \n",
    "        'objective':'reg:squarederror', \n",
    "        'learning_rate':0.01, \n",
    "    #     'early_stopping_rounds':6,\n",
    "        'min_child_weight':0.5,\n",
    "        'max_depth': 15\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "\n",
    "    return accuracy_score(test_y,pred), confusion_matrix(test_y, pred), model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 1 - ALL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.ARR_DEL15\n",
    "df = df.drop([\"ARR_DEL15\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN/TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y= train_test_split(df, target, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier - all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8323613412041632\n"
     ]
    }
   ],
   "source": [
    "accuracy_1, matrix_1, model_1 = XGB(train_x, train_y, test_x, test_y)\n",
    "print(accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.2114379 , 0.05719128, 0.1515516 , 0.03127415, 0.05412694,\n",
       "        0.06582016, 0.0554337 , 0.0775674 , 0.11127821, 0.04136954,\n",
       "        0.04708241, 0.        , 0.09586672], dtype=float32),\n",
       " Index(['MONTH', 'DAY_OF_WEEK', 'UNIQUE_CARRIER', 'FL_NUM', 'ORIGIN',\n",
       "        'ORIGIN_CITY_NAME', 'DEST', 'DEST_CITY_NAME', 'CRS_DEP_TIME',\n",
       "        'CRS_ELAPSED_TIME', 'DISTANCE', 'YEAR', 'DAY'],\n",
       "       dtype='object')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances_1 = []\n",
    "feature_importances_1.append(model_1.feature_importances_)\n",
    "feature_importances_1.append(train_x.columns)\n",
    "feature_importances.append(feature_importances_1)\n",
    "feature_importances_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 2 - CORRELATION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET DF FROM MASTER_DF\n",
    "df = master_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ARR_DEL15           1.000000\n",
       "CRS_DEP_TIME        0.139923\n",
       "DEST_CITY_NAME      0.033710\n",
       "CRS_ELAPSED_TIME    0.028205\n",
       "DISTANCE            0.026574\n",
       "DEST                0.025279\n",
       "YEAR                0.014014\n",
       "UNIQUE_CARRIER      0.012475\n",
       "ORIGIN              0.007675\n",
       "ORIGIN_CITY_NAME    0.005284\n",
       "DAY_OF_WEEK         0.003723\n",
       "DAY                -0.000719\n",
       "FL_NUM             -0.005766\n",
       "MONTH              -0.011563\n",
       "Name: ARR_DEL15, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = df.corr()\n",
    "corr_matrix['ARR_DEL15'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROP any values that are not uesful according to the coorelation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"MONTH\", 'FL_NUM', 'DAY'], axis=1).dropna() # tweaking\n",
    "\n",
    "\n",
    "target = df.ARR_DEL15\n",
    "df = df.drop([\"ARR_DEL15\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN/TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y= train_test_split(df, target, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier - correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8205972710414879\n"
     ]
    }
   ],
   "source": [
    "accuracy_2, matrix_2, model_2 = XGB(train_x, train_y, test_x, test_y)\n",
    "print(accuracy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.0289118 , 0.29406923, 0.05536764, 0.09881028, 0.07261533,\n",
       "        0.13281745, 0.11489715, 0.04442807, 0.07868834, 0.07939468],\n",
       "       dtype=float32),\n",
       " Index(['DAY_OF_WEEK', 'UNIQUE_CARRIER', 'ORIGIN', 'ORIGIN_CITY_NAME', 'DEST',\n",
       "        'DEST_CITY_NAME', 'CRS_DEP_TIME', 'CRS_ELAPSED_TIME', 'DISTANCE',\n",
       "        'YEAR'],\n",
       "       dtype='object')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances_2 = []\n",
    "feature_importances_2.append(model_2.feature_importances_)\n",
    "feature_importances_2.append(train_x.columns)\n",
    "feature_importances.append(feature_importances_2)\n",
    "feature_importances_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 3 - Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET DF FROM MASTER_DF\n",
    "df = master_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROP based on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features: %s [0, 2, 8, 12, 7, 5, 1, 6, 4, 10, 9, 3, 11]\n"
     ]
    }
   ],
   "source": [
    "important_features_dict = {}\n",
    "for x,i in enumerate(feature_importances_1[0]):\n",
    "    important_features_dict[x]=i\n",
    "\n",
    "\n",
    "important_features_list_1 = sorted(important_features_dict,\n",
    "                                 key=important_features_dict.get,\n",
    "                                 reverse=True)\n",
    "\n",
    "print('Most important features: %s', important_features_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features: %s [1, 5, 6, 3, 9, 8, 4, 2, 7, 0]\n"
     ]
    }
   ],
   "source": [
    "important_features_dict = {}\n",
    "for x,i in enumerate(feature_importances_2[0]):\n",
    "    important_features_dict[x]=i\n",
    "\n",
    "\n",
    "important_features_list_2 = sorted(important_features_dict,\n",
    "                                 key=important_features_dict.get,\n",
    "                                 reverse=True)\n",
    "\n",
    "print('Most important features: %s', important_features_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTH\n",
      "UNIQUE_CARRIER\n",
      "CRS_DEP_TIME\n",
      "DAY\n",
      "DEST_CITY_NAME\n",
      "ORIGIN_CITY_NAME\n",
      "DAY_OF_WEEK\n",
      "DEST\n",
      "ORIGIN\n",
      "DISTANCE\n",
      "CRS_ELAPSED_TIME\n",
      "FL_NUM\n",
      "YEAR\n"
     ]
    }
   ],
   "source": [
    "for i in important_features_list_1:\n",
    "    print(feature_importances_1[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUE_CARRIER\n",
      "DEST_CITY_NAME\n",
      "CRS_DEP_TIME\n",
      "ORIGIN_CITY_NAME\n",
      "YEAR\n",
      "DISTANCE\n",
      "DEST\n",
      "ORIGIN\n",
      "CRS_ELAPSED_TIME\n",
      "DAY_OF_WEEK\n"
     ]
    }
   ],
   "source": [
    "for i in important_features_list_2:\n",
    "    print(feature_importances_2[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['YEAR', 'DAY_OF_WEEK', 'CRS_ELAPSED_TIME', 'FL_NUM', 'ORIGIN'], axis=1).dropna() # tweaking\n",
    "\n",
    "\n",
    "target = df.ARR_DEL15\n",
    "df = df.drop([\"ARR_DEL15\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN/TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y= train_test_split(df, target, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier - Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8316140604757029\n"
     ]
    }
   ],
   "source": [
    "accuracy_3, matrix_3, model_3 = XGB(train_x, train_y, test_x, test_y)\n",
    "print(accuracy_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.26957694, 0.18738425, 0.07413484, 0.07303786, 0.09943362,\n",
       "        0.1222989 , 0.06016853, 0.11396504], dtype=float32),\n",
       " Index(['MONTH', 'UNIQUE_CARRIER', 'ORIGIN_CITY_NAME', 'DEST', 'DEST_CITY_NAME',\n",
       "        'CRS_DEP_TIME', 'DISTANCE', 'DAY'],\n",
       "       dtype='object')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances_3 = []\n",
    "feature_importances_3.append(model_3.feature_importances_)\n",
    "feature_importances_3.append(train_x.columns)\n",
    "feature_importances.append(feature_importances_3)\n",
    "feature_importances_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features: %s [0, 1, 5, 7, 4, 2, 3, 6]\n",
      "MONTH\n",
      "UNIQUE_CARRIER\n",
      "CRS_DEP_TIME\n",
      "DAY\n",
      "DEST_CITY_NAME\n",
      "ORIGIN_CITY_NAME\n",
      "DEST\n",
      "DISTANCE\n"
     ]
    }
   ],
   "source": [
    "important_features_dict = {}\n",
    "for x,i in enumerate(feature_importances_3[0]):\n",
    "    important_features_dict[x]=i\n",
    "\n",
    "\n",
    "important_features_list_3 = sorted(important_features_dict,\n",
    "                                 key=important_features_dict.get,\n",
    "                                 reverse=True)\n",
    "\n",
    "print('Most important features: %s', important_features_list_3)\n",
    "\n",
    "for i in important_features_list_3:\n",
    "    print(feature_importances_3[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of three different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCM(accuracy, matrix):\n",
    "#     import seaborn as sns\n",
    "\n",
    "#     print(\"XGBClassifier\")\n",
    "    print(\"ACCURACY SCORE: \", accuracy)\n",
    "#     print(\"\\n\\n\")\n",
    "    print(\"CONFUSION MATRIX: \")\n",
    "    print(matrix)\n",
    "#     sns.heatmap(matrix, fmt='.5g', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY SCORE:  0.8323613412041632\n",
      "CONFUSION MATRIX: \n",
      "[[615666   6602]\n",
      " [120594  15889]]\n"
     ]
    }
   ],
   "source": [
    "printCM(accuracy_1, matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY SCORE:  0.8205972710414879\n",
      "CONFUSION MATRIX: \n",
      "[[619033   3235]\n",
      " [132887   3596]]\n"
     ]
    }
   ],
   "source": [
    "printCM(accuracy_2, matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY SCORE:  0.8316140604757029\n",
      "CONFUSION MATRIX: \n",
      "[[615613   6655]\n",
      " [121108  15375]]\n"
     ]
    }
   ],
   "source": [
    "printCM(accuracy_3, matrix_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion - Approach 3 is the best with an accuracy of 83.16%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did 3 appraches to achieve my goal, first I encoded all the data into numbers to feed it to my model without any issues.  I then made two new columns, \"YEAR\" and \"DAY\" to extract the information from the \"FL_DATE\" column.  After this, I proceeded to drop the \"Unnamed: 13\" and \"FL_DATE\" columns as they were useless to use in my model.  Once I completed cleaning my dataframe, I then applied my 3 methods on the data. \n",
    "\n",
    "\n",
    "I choose to use a classifier as my goal was to predict a boolean state as the outcome.  Selecting a classifier was a real issue as I initially tried Random Forest Trees Classification and this took too long for my computer to run. This lead me to googling what alternative classifiers existed for large datasets (5 Mil+) and found that an XGBClassifier was a good approach for me to use.  I tested out the performance between the two models with a sample size of 1000000 and got a better runtime and accuracy from the XGBClassifier.  Hence this became my model to use for the whole data challenge. \n",
    "\n",
    "\n",
    "The first approach was initially just to see how well the model performed without any modification, it was surprisingly scoring at a good accuracy.  I stored its feature importances into a list to later utilize.  My second appraoch was to use the correlation matrix output from the datafram for the target column \"ARR_DEL15\", this lead me to drop columns that gave a negative value or a NaN value.  After doing this, I reapplied my model and the accuracy was slightly worse than the first approach.  For this approach I also saved the feature importances into a new list.  Finally, my final apprached combined the first two approaches' importance features to get a dataframe that kept only the column/features that had the most significant influence in my model.  \n",
    "\n",
    "\n",
    "\n",
    "Overall, my third and final approach scored a near same accuracy as my first approach however, I utilized a fraction of the columns/features in my final approach than in my first.  Additionally, viewing the confusion matrix I can also see that both approaches have a near same matrix with some slight variation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
